{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KlLeVhXEn7mQZ5v8zYTnPcbxU1fiAw8w",
      "authorship_tag": "ABX9TyPYcUPqXErfoMFW/HSKfuRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushpendracse/DataAnalyst/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyphen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28UBhG6BGQb_",
        "outputId": "771be348-7953-4491-a370-5ce71eb117e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (0.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7o5UQ0hNeYy",
        "outputId": "75c5982a-87a5-495a-cd27-cfb935bb1e43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.18.0.post0)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maxIB-ZfFeOD",
        "outputId": "7daf10bb-0dc5-4475-c6fe-8d4471a52058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Output saved as 'Output.xlsx'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from textblob import download_corpora\n",
        "import textblob\n",
        "textblob.download_corpora\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import pyphen\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "positive_words = {\"good\", \"happy\", \"excellent\", \"positive\", \"joy\", \"love\", \"fortunate\", \"pleasant\", \"great\", \"wonderful\"}\n",
        "negative_words = {\"bad\", \"sad\", \"terrible\", \"negative\", \"anger\", \"hate\", \"unfortunate\", \"horrible\", \"poor\", \"awful\"}\n",
        "\n",
        "\n",
        "syllable_counter = pyphen.Pyphen(lang='en')\n",
        "\n",
        "\n",
        "def extract_article(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.find('title').get_text()\n",
        "        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def compute_text_metrics(text):\n",
        "    blob = TextBlob(text)\n",
        "    positive_score = sum(1 for word in blob.words if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in blob.words if word.lower() in negative_words)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
        "    complex_word_count = sum(1 for word in words if syllable_counter.inserted(word).count('-') + 1 > 2)\n",
        "    percentage_complex_words = (complex_word_count / len(words) * 100) if words else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    syllables_per_word = sum(syllable_counter.inserted(word).count('-') + 1 for word in words) / len(words) if words else 0\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
        "\n",
        "    return {\n",
        "        \"POSITIVE_SCORE\": positive_score,\n",
        "        \"NEGATIVE_SCORE\": negative_score,\n",
        "        \"POLARITY_SCORE\": polarity,\n",
        "        \"SUBJECTIVITY_SCORE\": subjectivity,\n",
        "        \"AVG_SENTENCE_LENGTH\": avg_sentence_length,\n",
        "        \"PERCENTAGE_OF_COMPLEX_WORDS\": percentage_complex_words,\n",
        "        \"FOG_INDEX\": fog_index,\n",
        "        \"SYLLABLE_PER_WORD\": syllables_per_word,\n",
        "        \"WORD_COUNT\": word_count,\n",
        "        \"AVG_WORD_LENGTH\": avg_word_length,\n",
        "        \"PERSONAL_PRONOUNS\": personal_pronouns\n",
        "    }\n",
        "\n",
        "\n",
        "input_file = pd.read_excel('/content/drive/MyDrive/Input.xlsx')\n",
        "urls = input_file['URL']\n",
        "url_ids = input_file['URL_ID']\n",
        "\n",
        "\n",
        "for url, url_id in zip(urls, url_ids):\n",
        "    title, article = extract_article(url)\n",
        "    if title and article:\n",
        "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"{title}\\n\\n{article}\")\n",
        "\n",
        "\n",
        "output_data = []\n",
        "for url_id in url_ids:\n",
        "    try:\n",
        "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "        metrics = compute_text_metrics(text)\n",
        "        output_data.append({\"URL_ID\": url_id, **metrics})\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file for URL_ID {url_id}: {e}\")\n",
        "\n",
        "\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_excel('Output.xlsx', index=False)\n",
        "\n",
        "print(\"Processing complete. Output saved as 'Output.xlsx'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nw68Jjq4GZ9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}